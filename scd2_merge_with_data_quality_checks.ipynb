{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a960c088-47fa-4bfe-a922-e62dc9879b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, sum as _sum\n",
    "from delta.tables import DeltaTable\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "import os\n",
    "\n",
    "print(os.environ['SPARK_VERSION'])\n",
    "#  geting date parameter from databricks\n",
    "date_str = dbutils.widgets.get(\"arrival_date\")\n",
    "\n",
    "# if we hard code\n",
    "# date_str = \"2024-07-25\"\n",
    "\n",
    "\n",
    "# Define file paths based on date parameter\n",
    "booking_data = f\"/Volumes/incremental_data_load/default/orders_data/bookings_data/bookings_{date_str}.csv\"\n",
    "customer_data = f\"/Volumes/incremental_data_load/default/orders_data/customer_daily_data/customers_{date_str}.csv\"\n",
    "print(booking_data)\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831a1040-4f78-4df8-bc25-6a890c5b7b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read booking data\n",
    "booking_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"quote\",\"\\\"\") \\\n",
    "    .option(\"multiLine\",\"true\") \\\n",
    "    .load(booking_data)\n",
    "\n",
    "booking_df.printSchema()\n",
    "display(booking_df)\n",
    "\n",
    "# Read customer data for scd2 merge\n",
    "customer_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"quote\",\"\\\"\") \\\n",
    "    .option(\"multiLine\",\"true\") \\\n",
    "    .load(customer_data)\n",
    "\n",
    "customer_df.printSchema()\n",
    "display(customer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e57881-e5cc-446f-9a39-084b707c17db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Quality Checks on booking data\n",
    "check_incremental = Check(spark, CheckLevel.Error, \"Booking Data Check\") \\\n",
    "    .hasSize(lambda x: x > 0) \\\n",
    "    .isUnique(\"booking_id\", hint=\"Booking ID is not unique\") \\\n",
    "    .isComplete(\"customer_id\") \\\n",
    "    .isNonNegative(\"amount\") \\\n",
    "    .isNonNegative(\"quantity\") \\\n",
    "    .isNonNegative(\"discount\")\n",
    "\n",
    "# Data Quality Checks on customer data\n",
    "# check_scd = Check(spark, CheckLevel.Error, \"Customer Data Check\") \\\n",
    "#     .hasSize(lambda x: x > 0) \\\n",
    "#     .isUnique(\"customer_id\") \\\n",
    "#     .isComplete(\"customer_name\") \\\n",
    "#     .isComplete(\"customer_address\") \\\n",
    "#     .isComplete(\"phone_number\") \\\n",
    "#     .isComplete(\"email\")\n",
    "\n",
    "check_scd = Check(spark, CheckLevel.Error, \"Customer Data Check\") \\\n",
    "    .hasSize(lambda x: x > 0) \\\n",
    "    .isUnique(\"customer_id\") \\\n",
    "    .isComplete(\"customer_name\") \\\n",
    "    .isComplete(\"customer_address\") \\\n",
    "    .isComplete(\"email\")\n",
    "\n",
    "\n",
    "# Run the verification suite\n",
    "booking_dq_check = VerificationSuite(spark) \\\n",
    "    .onData(booking_df) \\\n",
    "    .addCheck(check_incremental) \\\n",
    "    .run()\n",
    "\n",
    "customer_dq_check = VerificationSuite(spark) \\\n",
    "    .onData(customer_df) \\\n",
    "    .addCheck(check_scd) \\\n",
    "    .run()\n",
    "\n",
    "booking_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, booking_dq_check)\n",
    "display(booking_dq_check_df)\n",
    "\n",
    "customer_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, customer_dq_check)\n",
    "display(customer_dq_check_df)\n",
    "\n",
    "# Check if verification passed\n",
    "if booking_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data Quality Checks Failed for Booking Data\")\n",
    "\n",
    "if customer_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data Quality Checks Failed for Customer Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a444a28-8ba2-41f2-8c62-e974515f2ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add ingestion timestamp to booking data\n",
    "booking_df_incremental = booking_df.withColumn(\"ingestion_time\", current_timestamp())\n",
    "\n",
    "# Join booking data with customer data\n",
    "df_joined = booking_df_incremental.join(customer_df, \"customer_id\")\n",
    "\n",
    "# Business transformation: calculate total cost after discount and filter\n",
    "df_transformed = df_joined \\\n",
    "    .withColumn(\"total_cost\", col(\"amount\") - col(\"discount\")) \\\n",
    "    .filter(col(\"quantity\") > 0)\n",
    "\n",
    "# Group by and aggregate df_transformed\n",
    "df_transformed_agg = df_transformed \\\n",
    "    .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_cost\").alias(\"total_amount_sum\"),\n",
    "        _sum(\"quantity\").alias(\"total_quantity_sum\")\n",
    "    )\n",
    "\n",
    "display(df_transformed_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b556d7eb-3859-4f33-8ffc-44c69de4941b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the Delta table exists\n",
    "fact_table_path = \"incremental_data_load.default.booking_fact\"\n",
    "fact_table_exists = spark._jsparkSession.catalog().tableExists(fact_table_path)\n",
    "\n",
    "\n",
    "if fact_table_exists:\n",
    "    # Read the existing fact table\n",
    "    df_existing_fact = spark.read.format(\"delta\").table(fact_table_path)\n",
    "    \n",
    "    # Combine the aggregated data\n",
    "    df_combined = df_existing_fact.unionByName(df_transformed_agg, allowMissingColumns=True)\n",
    "    \n",
    "    # Perform another group by and aggregation on the combined data\n",
    "    df_final_agg = df_combined \\\n",
    "        .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "        .agg(\n",
    "            _sum(\"total_amount_sum\").alias(\"total_amount_sum\"),\n",
    "            _sum(\"total_quantity_sum\").alias(\"total_quantity_sum\")\n",
    "        )\n",
    "else:\n",
    "    # If the fact table doesn't exist, use the aggregated transformed data directly\n",
    "    df_final_agg = df_transformed_agg\n",
    "\n",
    "display(df_final_agg)\n",
    "\n",
    "\n",
    "# Write the final aggregated data back to the Delta table\n",
    "df_final_agg.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(fact_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e0fd93-03b3-429b-ad8b-feb5bb4cbd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scd_table_path = \"incremental_data_load.default.customer_dim\"\n",
    "scd_table_exists = spark._jsparkSession.catalog().tableExists(scd_table_path)\n",
    "\n",
    "# Check if the customers table exists\n",
    "if scd_table_exists:\n",
    "    # Load the existing SCD table\n",
    "    scd_table = DeltaTable.forName(spark, scd_table_path)\n",
    "    display(scd_table.toDF())\n",
    "    \n",
    "    # Perform SCD2 merge logic\n",
    "    scd_table.alias(\"scd\") \\\n",
    "        .merge(\n",
    "            customer_df.alias(\"updates\"),\n",
    "            \"scd.customer_id = updates.customer_id and scd.valid_to = '9999-12-31'\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"valid_to\": \"updates.valid_from\",\n",
    "        }) \\\n",
    "        .execute()\n",
    "\n",
    "    customer_df.write.format(\"delta\").mode(\"append\").saveAsTable(scd_table_path)\n",
    "else:\n",
    "    # If the SCD table doesn't exist, write the customer data as a new Delta table\n",
    "    customer_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(scd_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a56460a-5615-464a-b309-05292f4b8002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scd2_merge_with_data_quality_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
